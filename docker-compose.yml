# Original file : https://github.com/datahub-project/datahub/blob/master/docker/docker-compose.yml
# All docker-related files : https://github.com/datahub-project/datahub/tree/master/docker
---
version: '3.8'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:5.4.0
    env_file: .env
    hostname: zookeeper
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: ${ZOOKEEPER_CLIENT_PORT}
      ZOOKEEPER_TICK_TIME: ${ZOOKEEPER_TICK_TIME}
    ports:
      - "2181:2181"
    volumes:
      - zk_data:/var/opt/zookeeper

  broker:
    image: confluentinc/cp-kafka:5.4.0
    env_file: .env
    hostname: broker
    container_name: broker
    environment:
      KAFKA_HEAP_OPTS: ${KAFKA_HEAP_OPTS}
      KAFKA_BROKER_ID: ${KAFKA_BROKER_ID}
      KAFKA_ZOOKEEPER_CONNECT: ${KAFKA_ZOOKEEPER_CONNECT}
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: ${KAFKA_LISTENER_SECURITY_PROTOCOL_MAP}
      KAFKA_ADVERTISED_LISTENERS: ${KAFKA_ADVERTISED_LISTENERS}
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: ${KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR}
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: ${KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS}
    depends_on:
      - zookeeper
    ports:
      - "29092:29092"
      - "9092:9092"
    volumes:
      - kafka_data:/var/lib/kafka/data/

  # This "container" is a workaround to pre-create topics
  kafka-setup:
    image: linkedin/datahub-kafka-setup:${DATAHUB_VERSION}
    env_file: .env
    hostname: kafka-setup
    container_name: kafka-setup
    environment:
      KAFKA_BOOTSTRAP_SERVER: ${KAFKA_BOOTSTRAP_SERVER}
      KAFKA_SCHEMAREGISTRY_URL: ${KAFKA_SCHEMAREGISTRY_URL}
    depends_on:
      - broker
      - schema-registry

  schema-registry:
    image: confluentinc/cp-schema-registry:5.4.0
    env_file: .env
    hostname: schema-registry
    container_name: schema-registry
    environment:
      SCHEMA_REGISTRY_HOST_NAME: ${SCHEMA_REGISTRY_HOST_NAME}
      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: ${SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL}
    depends_on:
      - zookeeper
      - broker
    ports:
      - "8081:8081"

  elasticsearch:
    image: elasticsearch:7.9.3
    env_file: .env
    container_name: elasticsearch
    hostname: elasticsearch
    ports:
      - "9200:9200"
    environment:
      discovery.type: single-node
      xpack.security.enabled: "false"
      ES_JAVA_OPTS: ${ES_JAVA_OPTS}
    volumes:
      - es_data:/usr/share/elasticsearch/data
    healthcheck:
        test: ["CMD-SHELL", "curl -sS --fail 'http://localhost:9200/_cluster/health?wait_for_status=yellow&timeout=0s' || exit 1"]
        start_period: 2m
        retries: 4

  neo4j:
    image: neo4j:4.0.6
    env_file: .env
    hostname: neo4j
    container_name: neo4j
    environment:
      NEO4J_AUTH: ${NEO4J_USERNAME}/${NEO4J_PASSWORD}
      NEO4J_dbms_default__database: ${NEO4J_dbms_default__database}
      NEO4J_dbms_allow__upgrade: ${NEO4J_dbms_allow__upgrade}
    ports:
      - "7474:7474"
      - "7687:7687"
    volumes:
      - neo4j_data:/data

  # This "container" is a workaround to pre-create search indices
  elasticsearch-setup:
    image: linkedin/datahub-elasticsearch-setup:${DATAHUB_VERSION}
    env_file: .env
    hostname: elasticsearch-setup
    container_name: elasticsearch-setup
    environment:
      ELASTICSEARCH_HOST: ${DATAHUB_GMS_ELASTICSEARCH_HOST}
      ELASTICSEARCH_PORT: ${DATAHUB_GMS_ELASTICSEARCH_PORT}
      ELASTICSEARCH_PROTOCOL: http
    depends_on:
      - elasticsearch

  datahub-gms:
    image: linkedin/datahub-gms:${DATAHUB_VERSION}
    hostname: datahub-gms
    container_name: datahub-gms
    env_file: .env
    environment:
      EBEAN_DATASOURCE_USERNAME: ${DATAHUB_GMS_EBEAN_DATASOURCE_USERNAME}
      EBEAN_DATASOURCE_PASSWORD: ${DATAHUB_GMS_EBEAN_DATASOURCE_PASSWORD}
      EBEAN_DATASOURCE_HOST: ${DATAHUB_GMS_EBEAN_DATASOURCE_HOST}
      EBEAN_DATASOURCE_URL: ${DATAHUB_GMS_EBEAN_DATASOURCE_URL}
      EBEAN_DATASOURCE_DRIVER: ${DATAHUB_GMS_EBEAN_DATASOURCE_DRIVER}
      ELASTICSEARCH_HOST: ${DATAHUB_GMS_ELASTICSEARCH_HOST}
      ELASTICSEARCH_PORT: ${DATAHUB_GMS_ELASTICSEARCH_PORT}
      MAE_CONSUMER_ENABLED: ${DATAHUB_GMS_MAE_CONSUMER_ENABLED}
      MCE_CONSUMER_ENABLED: ${DATAHUB_GMS_MCE_CONSUMER_ENABLED}
      JAVA_OPTS: ${DATAHUB_GMS_JAVA_OPTS}
      ENTITY_REGISTRY_CONFIG_PATH: ${DATAHUB_GMS_ENTITY_REGISTRY_CONFIG_PATH}
    ports:
      - "8080:8080"
    depends_on:
      - elasticsearch-setup
      - kafka-setup
      - datahub-gms-postgres
      - neo4j

  datahub-gms-postgres:
    container_name: datahub-gms-postgres
    hostname: datahub-gms-postgres
    image: postgres:12.3
    env_file: .env
    environment:
      POSTGRES_USER: ${DATAHUB_GMS_POSTGRES_USER}
      POSTGRES_PASSWORD: ${DATAHUB_GMS_POSTGRES_PASSWORD}
    ports:
      - '5432:5432'
    volumes:
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql

  datahub-frontend-react:
    image: linkedin/datahub-frontend-react:${DATAHUB_VERSION}
    env_file: .env
    hostname: datahub-frontend-react
    container_name: datahub-frontend-react
    environment:
      JAVA_OPTS: ${DATAHUB_FRONTEND_JAVA_OPTS}
      EBEAN_DATASOURCE_USERNAME: ${DATAHUB_GMS_EBEAN_DATASOURCE_USERNAME}
      EBEAN_DATASOURCE_PASSWORD: ${DATAHUB_GMS_EBEAN_DATASOURCE_PASSWORD}
      EBEAN_DATASOURCE_HOST: ${DATAHUB_GMS_EBEAN_DATASOURCE_HOST}
      EBEAN_DATASOURCE_URL: ${DATAHUB_GMS_EBEAN_DATASOURCE_URL}
      EBEAN_DATASOURCE_DRIVER: ${DATAHUB_GMS_EBEAN_DATASOURCE_DRIVER}
      KAFKA_BOOTSTRAP_SERVER: ${KAFKA_BOOTSTRAP_SERVER}
      KAFKA_SCHEMAREGISTRY_URL: ${KAFKA_SCHEMAREGISTRY_URL}
      ELASTIC_CLIENT_HOST: ${DATAHUB_GMS_ELASTICSEARCH_HOST}
      ELASTIC_CLIENT_PORT: ${DATAHUB_GMS_ELASTICSEARCH_PORT}
      NEO4J_HOST: ${NEO4J_HOST}
      NEO4J_URI: ${NEO4J_URI}
      NEO4J_USERNAME: ${NEO4J_USERNAME}
      NEO4J_PASSWORD: ${NEO4J_PASSWORD}
      MAE_CONSUMER_ENABLED: ${DATAHUB_GMS_MAE_CONSUMER_ENABLED}
      MCE_CONSUMER_ENABLED: ${DATAHUB_GMS_MCE_CONSUMER_ENABLED}
      DATAHUB_TRACKING_TOPIC: ${DATAHUB_FRONTEND_TRACKING_TOPIC}
    ports:
      - "9002:9002"
    depends_on:
      - datahub-gms
    volumes:
      - ${HOME}/.datahub/plugins:/etc/datahub/plugins

  datahub-actions:
    image: acryldata/acryl-datahub-actions:${ACTIONS_VERSION}
    hostname: actions
    env_file: .env
    restart: on-failure:5
    environment:
      GMS_HOST: ${DATAHUB_GMS_HOST}
      GMS_PORT: ${DATAHUB_GMS_PORT}
      KAFKA_BOOTSTRAP_SERVER: ${KAFKA_BOOTSTRAP_SERVER}
      SCHEMA_REGISTRY_URL: ${KAFKA_SCHEMAREGISTRY_URL}
      METADATA_AUDIT_EVENT_NAME: ${DATAHUB_ACTIONS_METADATA_AUDIT_EVENT_NAME}
      METADATA_CHANGE_LOG_VERSIONED_TOPIC_NAME: ${DATAHUB_ACTIONS_METADATA_CHANGE_LOG_VERSIONED_TOPIC_NAME}
      DATAHUB_SYSTEM_CLIENT_ID: ${DATAHUB_ACTIONS_SYSTEM_CLIENT_ID}
      DATAHUB_SYSTEM_CLIENT_SECRET: ${DATAHUB_ACTIONS_SYSTEM_CLIENT_SECRET}
      KAFKA_PROPERTIES_SECURITY_PROTOCOL: ${DATAHUB_ACTIONS_KAFKA_PROPERTIES_SECURITY_PROTOCOL}
    depends_on:
      - datahub-gms

volumes:
  es_data:
  neo4j_data:
  zk_data:
  kafka_data:
